{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian regression for storm erosion prediction\n",
    "Joshua Simmons 05/2022\n",
    "\n",
    "Further ideas:\n",
    " - https://nbviewer.org/github/pyro-ppl/numpyro/blob/master/notebooks/source/bayesian_regression.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pdb 1\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('talk')\n",
    "\n",
    "from ipywidgets import interactive, fixed, widgets\n",
    "\n",
    "# MAP regression\n",
    "import sklearn.linear_model as lm\n",
    "\n",
    "# pymc3\n",
    "\n",
    "# import pymc3 as pm\n",
    "\n",
    "# from pymc3 import HalfCauchy, Model, Normal, sample, HalfNormal\n",
    "\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download wave data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the most recent data\n",
    "rawWaveData = pd.read_csv(\n",
    "    os.path.join(\n",
    "        '/data',\n",
    "        'Waverider_buoys_Observations_-_Australia_-_near_real-time.csv'\n",
    "    ),\n",
    "    skiprows=16\n",
    ")\n",
    "rn_dict = {\n",
    "    'WHTH': 'Hsig',\n",
    "    'VDIR': 'Wdir',\n",
    "    'SPWP': 'Tp'\n",
    "}\n",
    "rawWaveData.rename(columns=rn_dict,inplace=True)\n",
    "\n",
    "rawWaveData = rawWaveData[['TIME','site_name','Hsig','Tp','Wdir']]\n",
    "\n",
    "wave_data_sorted = {\n",
    "    _: rawWaveData.query('site_name == \"{}\"'.format(_)) for _ in rawWaveData.site_name.unique()\n",
    "}\n",
    "\n",
    "for _ in wave_data_sorted.keys():\n",
    "    wave_data_sorted[_] = wave_data_sorted[_].set_index('TIME').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the most recent data\n",
    "rawWaveData = pd.read_csv(\n",
    "    os.path.join(\n",
    "        '/data',\n",
    "        'Waverider_buoys_Observations_-_Australia_-_delayed_National_Wave_Archive.csv'\n",
    "    ),\n",
    "    skiprows=99\n",
    ")\n",
    "rn_dict = {\n",
    "    'WHTH': 'Hsig',\n",
    "    'WPDI': 'Wdir',\n",
    "    'WPPE': 'Tp'\n",
    "}\n",
    "rawWaveData.rename(columns=rn_dict,inplace=True)\n",
    "\n",
    "rawWaveData = rawWaveData[['TIME','site_name','Hsig','Tp','Wdir']]\n",
    "\n",
    "wave_data_sorted = {\n",
    "    _: rawWaveData.query('site_name == \"{}\"'.format(_)) for _ in rawWaveData.site_name.unique()\n",
    "}\n",
    "\n",
    "for _ in wave_data_sorted.keys():\n",
    "    wave_data_sorted[_] = wave_data_sorted[_].set_index('TIME').sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawWaveData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_data_sorted['Sydney']['Hsig'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawWaveData = pd.read_csv('combined_era_data_-34.0_151.5.csv',index_col=0,parse_dates=True)\n",
    "rawWaveData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download shoreline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transectName = 'aus0206-0005'\n",
    "csSource = 'http://coastsat.wrl.unsw.edu.au/time-series/{}/'\n",
    "tmpLoc =  \"working_data.csv\"\n",
    "download = False\n",
    "\n",
    "if download:\n",
    "    urllib.request.urlretrieve(csSource.format(transectName), tmpLoc)\n",
    "\n",
    "rawShlData = pd.read_csv(tmpLoc,parse_dates=True,index_col=0,header=None)\n",
    "rawShlData.index = pd.to_datetime(rawShlData.index,utc=True)\n",
    "rawShlData.columns = ['Shoreline']\n",
    "rawShlData.index.name = 'Date'\n",
    "# shlData.plot()\n",
    "\n",
    "diffShlData = rawShlData.diff()\n",
    "diffShlData.columns = ['dShl'] \n",
    "\n",
    "sns.histplot(diffShlData.dropna())\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to storm dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wave_energy(waveIn):\n",
    "    rho = 1025 # kg/m^3\n",
    "    g = 9.81 # m/s^2\n",
    "    # this needs to be more robust\n",
    "    dt = (waveIn.index[1] - waveIn.index[0]).seconds/3600\n",
    "    stormCriteria = waveIn['Hsig']>3.0 #m\n",
    "    energy = np.sum(waveIn.loc[stormCriteria,'Hsig']**2)*rho*g*dt*(1/16)\n",
    "    return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the diffShlData df\n",
    "shlData = rawShlData.diff().copy()\n",
    "shlData['postDate'] = shlData.index\n",
    "shlData.loc[shlData.index[1:],'preDate'] = shlData.index[:-1]\n",
    "shlData.drop(shlData.index[0],inplace=True)\n",
    "shlData.index = ['Storm_{0:04.0f}'.format(_) for _ in range(shlData.shape[0])]\n",
    "shlData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the wave data according to the shoreline data\n",
    "waveData = {}\n",
    "for thisStorm in shlData.index:\n",
    "    waveData[thisStorm] = rawWaveData.loc[shlData.loc[thisStorm,'preDate']:shlData.loc[thisStorm,'postDate']]\n",
    "    shlData.loc[thisStorm,'E'] = calculate_wave_energy(waveData[thisStorm])\n",
    "waveData[thisStorm]\n",
    "shlData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constrain shlData by adding consective shoreline movements\n",
    "# print(shlData)\n",
    "shlData['zeroCross'] = np.sign(shlData['Shoreline']).diff().ne(0).astype(int)\n",
    "shlData['zeroCross'] = shlData['zeroCross'].cumsum()\n",
    "groupedVals = shlData.groupby(by='zeroCross',as_index=False).sum()\n",
    "groupedVals.index = shlData.index[[np.where(shlData['zeroCross'] == _)[0][0] for _ in groupedVals['zeroCross']]]\n",
    "shlData['E'] = groupedVals['E']\n",
    "shlData['Shoreline'] = groupedVals['Shoreline']\n",
    "shlData.drop_duplicates(subset=['zeroCross'],inplace=True)\n",
    "shlData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data\n",
    "plotData = shlData.dropna().copy()\n",
    "plotData = plotData.loc[(plotData['E']>250000) & (plotData['Shoreline']<-5)]\n",
    "plotData = plotData.drop(plotData.index[(plotData['E']>600000) & (plotData['Shoreline']>-20)])\n",
    "\n",
    "x, y = plotData['E'].values, -plotData['Shoreline'].values\n",
    "\n",
    "# and trasnform to logspace\n",
    "x_log = np.log(x)\n",
    "y_log = np.log(y)\n",
    "\n",
    "log_scale=False\n",
    "fig = plt.figure(figsize=(7, 5))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "sns.scatterplot(x='E',y='Shoreline',hue='preDate',data=plotData,ax=ax1)\n",
    "\n",
    "ax1.set_xlabel('E')\n",
    "ax1.set_ylabel('dShl')\n",
    "ax1.invert_yaxis()\n",
    "# change axes to log - log scale\n",
    "if log_scale:\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "plt.title('Shoreline change from satelite')\n",
    "ax1.get_legend().remove()\n",
    "# plt.legend(loc='center', bbox_to_anchor=(1.5,0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data that follows a power law distribution\n",
    "# y = a * x^b + error\n",
    "#defaults\n",
    "powa = 0.05\n",
    "constb = 1.5\n",
    "err_var = 0.15\n",
    "hs_scale = 0\n",
    "# Plot x, y data\n",
    "\n",
    "def generate_data(powa, constb, err_var, hs_scale, n_points, log_scale=False):\n",
    "    print('Choose a and b in y = a * x^b')\n",
    "    x = np.around(np.random.uniform(0, 10, n_points),decimals=2)\n",
    "\n",
    "    # generate the error\n",
    "    err = np.random.normal(0, err_var, len(x))\n",
    "    # add a homoscedastic error\n",
    "    err += (err * x) * hs_scale\n",
    "\n",
    "\n",
    "    y = (powa * x**constb) + err\n",
    "\n",
    "    plt.plot(x, y, 'o',label='Data')\n",
    "    xReal = np.linspace(0,10,100)\n",
    "    yReal = powa * xReal**constb\n",
    "    plt.plot(xReal,yReal,'r-',label='True function')\n",
    "\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    # change axes to log - log scale\n",
    "    if log_scale:\n",
    "        plt.xscale('log')\n",
    "        plt.yscale('log')\n",
    "    plt.title('Power Law Distribution')\n",
    "    plt.legend(loc='center', bbox_to_anchor=(1.1,0.5))\n",
    "    plt.show()\n",
    "\n",
    "    return x, y\n",
    "\n",
    "result = interactive(\n",
    "    generate_data,\n",
    "    powa = widgets.FloatText(value=powa,description='a',step=0.05), \n",
    "    constb = widgets.FloatText(value=constb,description='b',step=0.5),\n",
    "    err_var = widgets.FloatText(value=err_var,description='variance',step=0.05),\n",
    "    hs_scale = widgets.FloatText(value=hs_scale,description='homoscedasticity',step=0.05),\n",
    "    n_points = widgets.IntSlider(value=5,min=5,max=100,step=5,description='n_points'),\n",
    "    log_scale=False\n",
    ")\n",
    "\n",
    "display(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = result.result\n",
    "print('x: {}\\ny: {}'.format(x.shape,y.shape))\n",
    "\n",
    "# and trasnform to logspace\n",
    "x_log = np.log(x)\n",
    "y_log = np.log(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_fit(xObs,yObs,xPred,yPred,ySample=None,yMAP=None,**kwargs):\n",
    "    fig = plt.figure(figsize=(7, 5))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "\n",
    "    # if kwargs.get('errBands',False):\n",
    "    #     ax1.fill_between(xSample, predyMean - 1.96*predyStd, predyMean + 1.96*predyStd, alpha=0.2,label='95% CI')\n",
    "    #     ax1.plot(xSample, sampleyMean[:,0],label='Samples')\n",
    "    #     ax1.plot(xSample, sampleyMean[:,1:])\n",
    "    # else:\n",
    "    #     ax1.plot(xSample, sampleyMean)#,label='Samples')\n",
    "\n",
    "    if not ySample is None:\n",
    "        ax1.plot(xPred, ySample[0,:].T,'-',color='xkcd:light grey',alpha=0.4,label='Samples')\n",
    "        ax1.plot(xPred, ySample[1:,:].T,'-',color='xkcd:light grey',alpha=0.4)\n",
    "\n",
    "    if not yMAP is None:\n",
    "        ax1.plot(xPred, yMAP.T,'-',color='xkcd:red',label='MAP fit')\n",
    "\n",
    "    # # plot mean\n",
    "    # ax1.plot(xSample, predyMean, 'k', label='Mean')\n",
    "\n",
    "    # if not y is None:\n",
    "    ax1.plot(xObs, yObs, 'o',color='xkcd:dark grey',label='Observed')\n",
    "    ax1.plot(xPred, yPred, 'C0', label='Predicted')\n",
    "    \n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('y')\n",
    "\n",
    "    if kwargs.get('log_scale',False):\n",
    "        ax1.set_xscale('log')\n",
    "        ax1.set_yscale('log')\n",
    "\n",
    "    ax1.set_title(kwargs.get('title','Power Law Distribution Fit'))\n",
    "    # place legend outside the plot\n",
    "    plt.legend(loc='center', bbox_to_anchor=(1.2, 0.5))\n",
    "    plt.show()\n",
    "    return ax1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a Bayesian Linear Regression in logspace - MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a linear model to the data\n",
    "model = lm.LinearRegression()\n",
    "model.fit(x_log.reshape(-1,1),y_log)\n",
    "\n",
    "sample_x = np.linspace(x.min(),x.max(),101)\n",
    "\n",
    "y_pred = model.predict(np.log(sample_x).reshape(-1,1))\n",
    "\n",
    "draw_fit(x,y,sample_x,np.exp(y_pred),title='MAP Fit')\n",
    "\n",
    "draw_fit(x,y,sample_x,np.exp(y_pred),log_scale=True,title='MAP Fit Log-Log')\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit with pymc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Model() as model:  # model specifications in PyMC3 are wrapped in a with-statement\n",
    "    # Define priors\n",
    "    sigma = HalfCauchy(\"sigma\", beta=10, testval=1.0)\n",
    "    intercept = Normal(\"Intercept\",0,sigma=10)\n",
    "    x_coeff = Normal(\"x\", 0, sigma=10)\n",
    "\n",
    "    # Define likelihood\n",
    "    likelihood = Normal(\"y\", mu=intercept + x_coeff * x_log, sigma=sigma, observed=y_log)\n",
    "\n",
    "    # Inference!\n",
    "    # draw 3000 posterior samples using NUTS sampling\n",
    "    trace = sample(10000, return_inferencedata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "az.plot_trace(trace, figsize=(10, 7));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nSamples = 100\n",
    "\n",
    "# generate draws from the posterior\n",
    "eval_lm = lambda x, sample: sample[\"Intercept\"] + sample[\"x\"] * x\n",
    "if not isinstance(trace, list):\n",
    "    trace = trace.posterior.to_dataframe().to_dict(orient=\"records\")\n",
    "ySample = np.zeros((nSamples, len(sample_x)))\n",
    "for ii, rand_loc in enumerate(np.random.randint(0, len(trace), nSamples)):\n",
    "    rand_sample = trace[rand_loc]\n",
    "    tmpSample = eval_lm(np.log(sample_x), rand_sample)\n",
    "    ySample[ii,:] = tmpSample\n",
    "\n",
    "mapParams = pm.find_MAP(model=model)\n",
    "yMAP = eval_lm(np.log(sample_x), mapParams)\n",
    "\n",
    "draw_fit(\n",
    "    x,y,\n",
    "    sample_x,np.full(sample_x.shape, np.nan),\n",
    "    ySample=np.exp(ySample),yMAP=np.exp(yMAP),\n",
    "    title='Fit with uncertainty')\n",
    "\n",
    "draw_fit(\n",
    "    x,y,\n",
    "    sample_x,np.full(sample_x.shape, np.nan),\n",
    "    ySample=np.exp(ySample),yMAP=np.exp(yMAP),\n",
    "    log_scale=True,title='Log-Log fit with uncertainty')\n",
    "\n",
    "None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "87fbea7b3721842a93ed4da8a1ac4b18f42b1eaaedefc3a2702202c09bf233e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
